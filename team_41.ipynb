{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb34c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed50ed7-c24b-444a-9f46-a81c8d39519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70627e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a28ca6-9411-4b85-bc45-c9919e980570",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/dgxa_home/se22uari173/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c27d5a-fd2b-4e03-b786-f61d1fa8fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "cbbf8ed0-265c-4727-be7f-f2cac5a7842e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...</td>\n",
       "      <td>Mr. Smith's getting a check-up, and Doctor Haw...</td>\n",
       "      <td>get a check-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>#Person1#: Hello Mrs. Parker, how have you bee...</td>\n",
       "      <td>Mrs Parker takes Ricky for his vaccines. Dr. P...</td>\n",
       "      <td>vaccines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>#Person1#: Excuse me, did you see a set of key...</td>\n",
       "      <td>#Person1#'s looking for a set of keys and asks...</td>\n",
       "      <td>find keys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>#Person1#: Why didn't you tell me you had a gi...</td>\n",
       "      <td>#Person1#'s angry because #Person2# didn't tel...</td>\n",
       "      <td>have a girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>#Person1#: Watsup, ladies! Y'll looking'fine t...</td>\n",
       "      <td>Malik invites Nikki to dance. Nikki agrees if ...</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>train_12455</td>\n",
       "      <td>#Person1#: Excuse me. You are Mr. Green from M...</td>\n",
       "      <td>Tan Ling picks Mr. Green up who is easily reco...</td>\n",
       "      <td>pick up someone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>train_12456</td>\n",
       "      <td>#Person1#: Mister Ewing said we should show up...</td>\n",
       "      <td>#Person1# and #Person2# plan to take the under...</td>\n",
       "      <td>conference center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>train_12457</td>\n",
       "      <td>#Person1#: How can I help you today?\\n#Person2...</td>\n",
       "      <td>#Person2# rents a small car for 5 days with th...</td>\n",
       "      <td>rent a car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12458</th>\n",
       "      <td>train_12458</td>\n",
       "      <td>#Person1#: You look a bit unhappy today. What'...</td>\n",
       "      <td>#Person2#'s mom lost her job. #Person2# hopes ...</td>\n",
       "      <td>job losing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459</th>\n",
       "      <td>train_12459</td>\n",
       "      <td>#Person1#: Mom, I'm flying to visit uncle Lee'...</td>\n",
       "      <td>#Person1# asks for #Person2#'s idea of packing...</td>\n",
       "      <td>baggage pack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12460 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                           dialogue  \\\n",
       "0          train_0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n",
       "1          train_1  #Person1#: Hello Mrs. Parker, how have you bee...   \n",
       "2          train_2  #Person1#: Excuse me, did you see a set of key...   \n",
       "3          train_3  #Person1#: Why didn't you tell me you had a gi...   \n",
       "4          train_4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n",
       "...            ...                                                ...   \n",
       "12455  train_12455  #Person1#: Excuse me. You are Mr. Green from M...   \n",
       "12456  train_12456  #Person1#: Mister Ewing said we should show up...   \n",
       "12457  train_12457  #Person1#: How can I help you today?\\n#Person2...   \n",
       "12458  train_12458  #Person1#: You look a bit unhappy today. What'...   \n",
       "12459  train_12459  #Person1#: Mom, I'm flying to visit uncle Lee'...   \n",
       "\n",
       "                                                 summary              topic  \n",
       "0      Mr. Smith's getting a check-up, and Doctor Haw...     get a check-up  \n",
       "1      Mrs Parker takes Ricky for his vaccines. Dr. P...           vaccines  \n",
       "2      #Person1#'s looking for a set of keys and asks...          find keys  \n",
       "3      #Person1#'s angry because #Person2# didn't tel...  have a girlfriend  \n",
       "4      Malik invites Nikki to dance. Nikki agrees if ...              dance  \n",
       "...                                                  ...                ...  \n",
       "12455  Tan Ling picks Mr. Green up who is easily reco...    pick up someone  \n",
       "12456  #Person1# and #Person2# plan to take the under...  conference center  \n",
       "12457  #Person2# rents a small car for 5 days with th...         rent a car  \n",
       "12458  #Person2#'s mom lost her job. #Person2# hopes ...         job losing  \n",
       "12459  #Person1# asks for #Person2#'s idea of packing...       baggage pack  \n",
       "\n",
       "[12460 rows x 4 columns]"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7366c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3920850",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5490be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbe439",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [\n",
    "        token.text for token in doc if not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc284a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "percent = 97\n",
    "print(np.percentile([len(x) for x in df['dialogue']],percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c52f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile([len(x) for x in df['summary']],percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc143301",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_seq_length = 1584\n",
    "max_output_seq_length = 290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da716a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_list = list(df['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list = list(df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ce52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dialogue_list)):\n",
    "    dialogue_list[i] = preprocess_text(dialogue_list[i])\n",
    "for i in range(len(summary_list)):\n",
    "    summary_list[i] = preprocess_text(summary_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = 0\n",
    "for i in dialogue_list:\n",
    "    if(len(i)>maxi):\n",
    "        maxi = len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c623e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_validlength(paragraph,max_seq_length):\n",
    "    return(len(paragraph)<max_input_seq_length)\n",
    "valid_textsummaries_idx = []\n",
    "for i in range(len(df['summary'])):\n",
    "    dialogue,summary = dialogue_list[i],summary_list[i]\n",
    "    if(is_validlength(dialogue,max_input_seq_length) and is_validlength(summary,max_output_seq_length)):\n",
    "        valid_textsummaries_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4a9d1-a3f6-4a65-9fcc-a07d6b424d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12085"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_textsummaries_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9906a5a-d011-4763-9763-75af4a0f3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_list_new = []\n",
    "summary_list_new = []\n",
    "for i in valid_textsummaries_idx:\n",
    "    dialogue_list_new.append(dialogue_list[i])\n",
    "    summary_list_new.append(summary_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974146f-558f-4be6-b988-ea2ea27527e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74ac31-13aa-49ce-a680-c068eafa07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self,dialogues,summaries):\n",
    "        self.dialogues = dialogues\n",
    "        self.summaries = summaries\n",
    "    def __len__(self):\n",
    "        return len(self.summaries)\n",
    "    def __getitem__(self,j):\n",
    "        return self.dialogues[j],self.summaries[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e42aa-5324-4fde-967d-6da6c02e322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataset(dialogue_list_new,summary_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f3445-0728-4d4e-8035-456cac63e936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\")"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e90e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextDataset(dialogue_list_new, summary_list_new)\n",
    "batch_size = 1\n",
    "train_dataset = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "for batch in train_dataset:\n",
    "    dialogues,summaries = batch\n",
    "    print(summaries)\n",
    "    if(c==1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "719f2a13-96d3-4b12-b0b7-d41fe3126501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\""
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dialogue'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "49e2c692-ea22-467b-8fcf-2dcc364f06b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12085"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d79062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "for i in dialogue_list_new:\n",
    "    if(max<len(tokenizer(i)['input_ids'])):\n",
    "        max = len(tokenizer(i)['input_ids'])\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "for i in summary_list_new:\n",
    "    if(max<len(tokenizer(i)['input_ids'])):\n",
    "        max = len(tokenizer(i)['input_ids'])\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ade265",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e50cf-b6c4-4023-8915-43ef69ad45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "output_ids = []\n",
    "max_input_seq_length = 450\n",
    "max_output_seq_length = 450\n",
    "for batch in train_dataset:\n",
    "    if(len(batch)==2):\n",
    "        dialogues, summaries = batch \n",
    "        tokenized_dialogues = tokenizer(list(dialogues),max_length=max_input_seq_length,padding='max_length',return_tensors='pt',add_special_tokens=False)\n",
    "        tokenized_summaries = tokenizer(list(summaries),max_length=max_output_seq_length,padding='max_length',return_tensors='pt',add_special_tokens=True)\n",
    "        input_ids.append(tokenized_dialogues['input_ids'])\n",
    "        output_ids.append(tokenized_summaries['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "26f0caa7-e896-43c8-b246-02fd0d05f8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6043"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tokenized_inputs = torch.cat(input_ids,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5657b94-6cd9-44bb-83a6-9c4db4d96ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tokenized_outputs = torch.cat(output_ids,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "1385203f-7971-4efa-9a23-17bb6a96618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n",
      "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
      "        [ 1713,   345, 13515,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(input_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "89ee0d66-cd3b-4526-92f5-56e13da9a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tokenized_outputs = torch.cat(output_ids,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "95df1040-1283-4449-8f8d-121fb7951726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1713,   345, 13515,  ...,     0,     0,     0],\n",
       "        [ 1713,   345, 13515,  ...,     0,     0,     0],\n",
       "        [ 1713,   345, 13515,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 1713,   345, 13515,  ...,     0,     0,     0],\n",
       "        [ 1713,   345, 13515,  ...,     0,     0,     0],\n",
       "        [ 1713,   345, 13515,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "122df9f6-ff48-4612-90c8-7b146f906e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12085"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor_tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3d3cc-f59d-4e95-901a-1b5a03855723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e630f-4fa0-4984-ab04-7e17a6dc73b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadattention(nn.Module):\n",
    "    def __init__(self,weights_dim,n_heads):\n",
    "        super().__init__()\n",
    "        self.weights_dim = weights_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dimensions = weights_dim//n_heads\n",
    "        self.qkv_matrices = nn.Linear(weights_dim,3*weights_dim)\n",
    "        self.linear_layer = nn.Linear(weights_dim,weights_dim)\n",
    "    def forward(self,x,mask):\n",
    "        batch,maxseqlength,weight_dim = x.size()\n",
    "        qkv = self.qkv_matrices(x)\n",
    "        qkv = qkv.reshape(batch,maxseqlength,self.n_heads,3*self.head_dimensions)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q,k,v = qkv.chunk(3,dim=-1)\n",
    "        k_transpose = k.transpose(-1,-2)\n",
    "        product = torch.matmul(q,k_transpose)\n",
    "        product = product/math.sqrt(q.size()[-1])\n",
    "        if mask is not None:\n",
    "            product = product+mask\n",
    "        attention_scores = func.softmax(product,dim=-1)\n",
    "        attentionintoV = torch.matmul(attention_scores,v)\n",
    "        n = (self.n_heads*self.head_dimensions)\n",
    "        output = self.linear_layer(attentionintoV.reshape(batch,maxseqlength,n))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78a71e-869d-4051-896d-f0c6eaaa8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(torch.nn.Module):\n",
    "    def __init__(self,eps):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.eps = eps \n",
    "    def forward(self,inp):\n",
    "        mean = inp.mean(dim=-1, keepdim=True) \n",
    "        var = ((inp - mean) ** 2).mean(dim=-1, keepdim=True)\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "        y = (inp - mean) / std\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a78d7-9807-43a0-bd32-245d8128a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedfowardlayer(nn.Module):\n",
    "    def __init__(self,weights_dims,hidden_dim,drop_prob):\n",
    "        super(feedfowardlayer, self).__init__()\n",
    "        self.weights_dims = weights_dims\n",
    "        self.hidden_dims = hidden_dim\n",
    "        self.linearlayer1 = nn.Linear(self.weights_dims,self.hidden_dims)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linearlayer2 = nn.Linear(self.hidden_dims,self.weights_dims)\n",
    "        self.drop_probability = drop_prob\n",
    "        self.drop_neurons = nn.Dropout(p=self.drop_probability)\n",
    "    def forward(self,x):\n",
    "        out = self.linearlayer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop_neurons(out)\n",
    "        out = self.linearlayer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eeadfc-2dad-427a-ab7a-3d9960a567ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subencoder(nn.Module):  # its one of the encoder layers in pipeline of encoder\n",
    "    def __init__(self,weight_dim,ff_hidden,n_heads,drop_probability):\n",
    "        super(Subencoder,self).__init__()\n",
    "        self.attention = Multiheadattention(weight_dim,n_heads)\n",
    "        self.norm = LayerNormalization(1e-6)\n",
    "        self.dropout1 = nn.Dropout(p = drop_probability)\n",
    "        self.ffd = feedfowardlayer(weight_dim,ff_hidden,drop_probability)\n",
    "        self.norm2 = LayerNormalization(1e-6)\n",
    "        self.dropout2 = nn.Dropout(p = drop_probability)\n",
    "    def forward(self,x,mask):\n",
    "        residual = x\n",
    "        x = self.attention(x,mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm(x+residual)\n",
    "        residual = x\n",
    "        x = self.ffd(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x+residual)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc88b82-8597-470a-8108-b61f8d61698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialencoder(nn.Sequential): #it makes pipeline for multiple encoders\n",
    "    def foward(self,x,mask):\n",
    "        self.x = x\n",
    "        self.mask = mask\n",
    "        for module in self._modules.values():\n",
    "            x = module(self.x,self.mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858dfad-fcd9-44c4-acd2-99798ccd83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,weight_dim,ff_hidden,n_heads,layers,drop_probability):\n",
    "        super(Encoder, self).__init__()\n",
    "        encoded_layers = []\n",
    "        for k in range(layers): \n",
    "            l = Subencoder(weight_dim, ff_hidden,n_heads,drop_probability)\n",
    "            encoded_layers.append(l)\n",
    "        self.layers = nn.ModuleList(encoded_layers)\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83b6f1-de99-4bc4-aa97-a4d18fa49806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadcrossattention(nn.Module):\n",
    "    def __init__(self,weights_dim,n_heads):\n",
    "        super().__init__()\n",
    "        self.weights_dim = weights_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dimensions = weights_dim//n_heads\n",
    "        self.kv_matrices = nn.Linear(weights_dim,2*weights_dim)\n",
    "        self.q_matrix = nn.Linear(weights_dim,weights_dim)\n",
    "        self.linear_layer = nn.Linear(weights_dim,weights_dim)\n",
    "    def forward(self,y,x,mask):\n",
    "        batches,maxseqlength,weight_dim = x.size()\n",
    "        kv = self.kv_matrices(x)\n",
    "        q = self.q_matrix(y)\n",
    "        kv = kv.reshape(batches,maxseqlength,self.n_heads,2*self.head_dimensions)\n",
    "        kv = kv.permute(0,2,1,3)\n",
    "        k,v = kv.chunk(2,dim=-1)\n",
    "        q = q.reshape(batches,maxseqlength,self.n_heads,self.head_dimensions)\n",
    "        q = q.permute(0,2,1,3)\n",
    "        k_transpose = k.transpose(-1,-2)\n",
    "        product = torch.matmul(q,k_transpose)\n",
    "        product = product/math.sqrt(q.size()[-1])\n",
    "        if mask is not None:\n",
    "            product = product+mask\n",
    "        attention_scores = func.softmax(product,dim=-1)\n",
    "        attentionintoV = torch.matmul(attention_scores,v)\n",
    "        n = (self.n_heads*self.head_dimensions)\n",
    "        output = self.linear_layer(attentionintoV.reshape(batches,maxseqlength,n))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8980d2-0dfe-4232-a2cc-cce9921b958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maskedattention(nn.Module):\n",
    "    def __init__(self,weights_dim,n_heads):\n",
    "        super().__init__()\n",
    "        self.weights_dim = weights_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dimensions = weights_dim//n_heads\n",
    "        self.qkv_matrices = nn.Linear(weights_dim,3*weights_dim)\n",
    "        self.linear_layer = nn.Linear(weights_dim,weights_dim)\n",
    "    def forward(self,x,mask,padmask):\n",
    "        batch,maxseqlength,weight_dim = x.size()\n",
    "        qkv = self.qkv_matrices(x)\n",
    "        qkv = qkv.reshape(batch,maxseqlength,self.n_heads,3*self.head_dimensions)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q,k,v = qkv.chunk(3,dim=-1)\n",
    "        k_transpose = k.transpose(-1,-2)\n",
    "        product = torch.matmul(q,k_transpose)\n",
    "        product = product/math.sqrt(q.size()[-1])\n",
    "        if mask is not None:\n",
    "            product = product+mask+padmask\n",
    "        attention_scores = func.softmax(product,dim=-1)\n",
    "        attentionintoV = torch.matmul(attention_scores,v)\n",
    "        n = (self.n_heads*self.head_dimensions)\n",
    "        output = self.linear_layer(attentionintoV.reshape(batch,maxseqlength,n))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed20719-6cae-4728-b923-ece7ff9d048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subdecoder(nn.Module):\n",
    "    def __init__(self,weight_dim,ff_hidden,n_heads,drop_probability):\n",
    "        super(Subdecoder,self).__init__()\n",
    "        self.maskedattention = Maskedattention(weight_dim,n_heads)\n",
    "        self.norm = LayerNormalization(1e-6)\n",
    "        self.dropout1 = nn.Dropout(p = drop_probability)\n",
    "        self.Multicrossattention = Multiheadcrossattention(weight_dim,n_heads)\n",
    "        self.norm2 = LayerNormalization(1e-6)\n",
    "        self.dropout2 = nn.Dropout(p = drop_probability)\n",
    "        self.ffd = feedfowardlayer(weight_dim,ff_hidden,drop_probability)\n",
    "        self.norm3 = LayerNormalization(1e-6)\n",
    "        self.dropout3 = nn.Dropout(p = drop_probability)\n",
    "    def forward(self,x,y,mask,padmask,crossmask):\n",
    "        residual = y\n",
    "        y = self.maskedattention(y,mask,padmask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.norm(y+residual)\n",
    "        residual = y\n",
    "        y = self.Multicrossattention(y,x,crossmask)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.norm2(y+residual)\n",
    "        residual = y\n",
    "        y = self.ffd(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = self.norm3(y+residual)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f360abe-cd6c-4182-9030-c03ba9d7bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialdecoder(nn.Sequential):\n",
    "    def forward(self,x,y,mask,padmask,crossattentpadmask):\n",
    "        for module in self._modules.values():\n",
    "            y = module(x,y,mask,padmask,crossattentpadmask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10def64-b22d-4efb-a65f-9d3f3dfa874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,weight_dim,ff_hidden,n_heads,layers,drop_probability):\n",
    "        super().__init__()\n",
    "        decoded_layers = []\n",
    "        for k in range(layers):\n",
    "            l = Subdecoder(weight_dim,ff_hidden,n_heads,drop_probability)\n",
    "            decoded_layers.append(l)\n",
    "        self.layers = Sequentialdecoder(*[Subdecoder(weight_dim,ff_hidden,n_heads,drop_probability)])\n",
    "    def forward(self,x,y,mask,padmask,crossattentpadmask):\n",
    "        resdecoder = self.layers(x,y,mask,padmask,crossattentpadmask)\n",
    "        return resdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fac14-710b-48c7-b89b-979961a542fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9c711-d7ea-4e88-8bdf-33d22bcf69cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingWithFeatureTransformation(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, max_sequence_length):\n",
    "        super(PositionalEncodingWithFeatureTransformation, self).__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.output_dim = output_dim\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.positional_embeddings = nn.Parameter(torch.zeros(max_sequence_length, output_dim), requires_grad=True)\n",
    "        self.create_positional_encodings()\n",
    "    def create_positional_encodings(self):\n",
    "        even_i = torch.arange(0, self.output_dim, 2).float()\n",
    "        denominator = torch.pow(10000, even_i / self.output_dim)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        self.positional_embeddings.data = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)  \n",
    "        positional_encoded = self.positional_embeddings[:x.size(1), :].unsqueeze(0) \n",
    "        return x + positional_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_inputs = nn.Linear(512,512)\n",
    "embedding_matrix_outputs = nn.Linear(512,512)\n",
    "positional_embeddings = PositionalEncodingWithFeatureTransformation(input_dim=512,output_dim=512,max_sequence_length=450)\n",
    "optimizer = torch.optim.Adam(list(transformer.parameters()) + list(embedding_matrix_inputs.parameters()) + list(embedding_matrix_outputs.parameters()) + list(positional_embeddings.parameters()),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "word2vec = Word2VecEmbedding(32000,512)\n",
    "for i in range(30):\n",
    "    weights_dim = 512\n",
    "    ff_hidden = 2048\n",
    "    n_heads = 8\n",
    "    drop_probability = 0.15\n",
    "    num_layers = 3\n",
    "    batch_size = 1\n",
    "    for i in range(len(train_dataset)):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        current_batch_inputs = tensor_tokenized_inputs[start_idx:end_idx]  \n",
    "        current_batch_outputs = tensor_tokenized_outputs[start_idx:end_idx]\n",
    "    with torch.no_grad(): \n",
    "        X = word2vec(current_batch_inputs) \n",
    "    with torch.no_grad():\n",
    "        Y = word2vec(current_batch_outputs)\n",
    "    embedded_X = embedding_matrix_inputs(X)\n",
    "    embedded_Y = embedding_matrix_outputs(Y)\n",
    "    positional_embeddingsx = positional_embeddings(X) \n",
    "    final_X = embedded_X + positional_embeddingsx\n",
    "    positional_embeddingsy = positional_embeddings(Y) \n",
    "    final_Y = embedded_Y + positional_embeddingsy\n",
    "    src_mask = create_padding_mask(current_batch_inputs,1,450,8)\n",
    "    tgt_mask = create_padding_mask(current_batch_outputs,1,450,8)\n",
    "    lkahd_mask = create_lookahead_mask(450,8)\n",
    "    crossattention_mask = create_padding_mask_2(current_batch_outputs,450,8)\n",
    "    logits = transformer(final_X,final_Y,src_mask,tgt_mask,lkahd_mask,crossattention_mask)\n",
    "    probabilities = func.softmax(logits,dim=-1)\n",
    "    predicted_tokens = torch.argmax(probabilities,dim=-1)\n",
    "    predicted_text = tokenizer.decode(predicted_tokens[0].tolist())\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), current_batch_outputs.view(-1))\n",
    "    print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
